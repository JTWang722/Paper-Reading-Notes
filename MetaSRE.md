# Semi-supervised Relation Extraction via Incremental Meta Self-Training

jtwang  2023/1/4

> Hu X, Zhang C, Ma F, et al. Semi-supervised relation extraction via incremental meta self-training[J]. arXiv preprint arXiv:2010.16410, 2020. EMNLP 2021.
> Paper link: https://arxiv.org/pdf/2010.16410.pdf
> Code link: https://github.com/THU-BPM/MetaSRE

# Abstract

- Self-training
- meta learning
- Relation extraction



# Introduction

- Foucs on **Relation extraction** problem
- While in practice, these labeled data would be laborintensive to obtain and error-prone
- A lot of work is being explored to alleviate the human supervision in relation extraction.
  - **Distant Supervision** methods, leverage external knowledge bases to obtain annotated triplets as the supervision
  - **Semi-Supervised** Relation Extraction method, 
    - self-ensembling
    - self-training, gradual drift problem


- To alleviate the noise in pseudo labels, we designa method that can **generate high-quality pseudo labels** from the unlabeled dataset as additional supervision. 
  - We propose a semi-supervised learning framework which adopts **meta-learning** during pseudo label generation and automatically learns to reduce the influence of noisy pseudo labels. 
  - We also adopt a **pseudo label selection and exploitation scheme** during each self-training step to take full advantage of high-quality pseudo labels


- The proposed framework has two networks, 
  - **Relation Classification Network (RCN)**, encodes entity pair representations based on the context in which they are mentioned, and train a classifier with both labeled data and pseudo-labeled data generated by RLGN to classify the representations into relation categories.
  - **Relation Label Generation Network (RLGN)**,  leverages pseudo label selection/exploitation scheme to calibrate confidence scores and obtain high-quality pseudo labels from unlabeled data for RCN
  - RLGN is meta optimized by RCN using the labeled data

![图 1](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_1.png)  


- Contributions
  - We propose a novel semi-supervised relation extraction framework MetaSRE (Incremental Meta Self-training Relation Extraction), 
  - We develop a label selection and exploitation scheme 


## 2 Proposed Model

#### 2.1 Relation Classification Network (RCN)

- The main purpose of RCN is to extract the relational representation of context from sentences, and classify these features to get the corresponding relations.
- the named entities in the sentences have been recognized and marked in advance, and we need to focus on the binary relations which involve two entities.
- Input: labeled data, [Sentence, Entity1, Entity2, Relation]
- **(1) Contextualized Relation Encoder**
  - BERT, to extract contextualized entity features
  - Input: enhance the position information of entities. For each sentence X, four reserved tokens [E1start], [E1end], [E2start], [E2end] are inserted

  ![图 2](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_2.png)  

  - Output: contextualized entity pair representation
![图 3](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_3.png)  

- **(2) Relation Classification**
- Loss, cross entropy, N golden labels and M pseudo labels
![图 4](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_4.png)  


#### 2.2 Relation Label Generation Network (RLGN)

- same architecture as the RCN
- is trained separately as a meta learner to label the unlabeled sentences and learn more about the distribution of relation mentions on pseudo labels through the RCN.
- The reason why we discard the classification network and retrain another network is to **prevent the noise contained in the generated pseudo labels** which would lead the sentence feature distribution to drift gradually
- RLGN is tuned to generate pseudo labels using only the labeled data
- RCN and RLGN have the same network structure but are initialized separately and trained completely independently
- Meta objective, the parameters of RLGN is $\eta$
  ![图 5](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_5.png)  
    - $\tau^+$, the parameters of the RCN after one gradient update using the loss Eq(4)
  ![图 6](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_6.png)  

    - $n$, N golden labels
    - The trick in meta objective is to use the updated parameters $\tau^+$ to calculate the derivative and update $\eta$

- Loss
![图 7](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_7.png)  


#### 2.3 Pseudo label Selection and Exploitation Selection

- Selection
  - Use the corresponding probability as the confidence score
  - Select the top Z% high-confidence pseudo labels as the final M pseudo labels

- Exploitation
  - Use the maximum probability value from the output of RLGN as the weight
  ![图 8](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_8.png)  

- Incremental Self-Training
  - We explore an **incremental way** to select high-quality pseudo labels and utilize them in batches to let the RCN gradually improve as we obtain more highquality pseudo labels from the RLGN

![图 9](fig/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training/Semi-supervised%20Relation%20Extraction%20via%20Incremental%20Meta%20Self-Training_9.png)  
