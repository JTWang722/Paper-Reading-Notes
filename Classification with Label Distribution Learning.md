# Classification with Label Distribution Learning

jtwang  2023/1/3

> Wang J, Geng X. Classification with Label Distribution Learning[C] //IJCAI. 2019: 3712-3718.
> 论文链接：https://www.ijcai.org/proceedings/2019/0515.pdf

## Introduction

- both SLL and MLL consider the relation between instance and label to be binary, i.e., whether or not the label is relevant with the instance
- However, there are a variety of real-world tasks that instances are involved with labels with different importance degree
- a soft label instead of a hard one seems to be a reasonable solution.
- LDL tackles label ambiguity with the definition of label description degree.
- According to the **source of label distribution**, applications can be mainly classified into three classes
  - From the data. 
  - Generated by pre-knowledge
  - Learned from the data


- A learned LDL model is generally treated as a classification model, with the label corresponding to the highest model output as the prediction, which unfortunately draws an **inconsistency between the aim of the training phrase and the goal of test phrase** of LDL.
  -  In the training phrase, the aim of LDL is to minimize the distance between the model output and the ground-truth label distribution
  - In the test phrase, the object is to minimize the 0/1 error



- In order to alleviate the inconsistency, we come up with three improvements. **LDL4C**
  - absolute loss, 
  - re-weighting with entropy information
  - large margin



总结：一堆公式推导，误差上界下届的理论证明